{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c4e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tagal\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.0526 - loss: 4.3291\n",
      "Epoch 2/2\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0506 - loss: 4.3232  \n",
      "Context words: ['deep', 'learning', 'known', 'as'] -> Target word: also\n",
      "Context words: ['learning', 'also', 'as', 'deep'] -> Target word: known\n",
      "Context words: ['also', 'known', 'deep', 'structured'] -> Target word: as\n",
      "Context words: ['known', 'as', 'structured', 'learning'] -> Target word: deep\n",
      "Context words: ['as', 'deep', 'learning', 'is'] -> Target word: structured\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# Sample data\n",
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\"\"\"\n",
    "\n",
    "# Data preparation\n",
    "sentences = data.lower().split('.')\n",
    "clean_sent = []\n",
    "for sentence in sentences:\n",
    "    if sentence.strip():  # Check for non-empty sentence,and remove the staring and ending whitespace if it is not having any string present then it will proceed\n",
    "        sentence = re.sub('[^a-zA-Z\\s]', '', sentence)  # Remove non-alphabetic characters\n",
    "        clean_sent.append(sentence.strip())\n",
    "\n",
    "# Generate sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_sent)\n",
    "sequences = tokenizer.texts_to_sequences(clean_sent)\n",
    "\n",
    "# Mapping words to indexes\n",
    "index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "emb_size = 10\n",
    "context_size = 2\n",
    "\n",
    "# Generate training data for CBOW\n",
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(context_size, len(sequence) - context_size):\n",
    "        target = sequence[i]\n",
    "        context = []\n",
    "        # Collect context words within the window size\n",
    "        for j in range(-context_size, context_size + 1):\n",
    "            if j != 0:\n",
    "                context.append(sequence[i + j])\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert the contexts and targets to numpy arrays\n",
    "X = np.array(contexts)\n",
    "Y = np.array(targets)\n",
    "\n",
    "# Model training\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=context_size * 2))  # CBOW needs context size * 2\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=2, verbose=1)\n",
    "\n",
    "# Print context and target pairs after training\n",
    "for i in range(5):  # Print the first 5 examples\n",
    "    words = [index_to_word[j] for j in contexts[i]]\n",
    "    target = index_to_word[targets[i]]\n",
    "    print(f\"Context words: {words} -> Target word: {target}\")\n",
    "\n",
    "# Output\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137adad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
