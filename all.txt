1

#installations
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import mnist
from tensorflow.keras import backend as K
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings("ignore")
#grabbing the mnist dataset
((X_train, Y_train), (X_test, Y_test)) = mnist.load_data()
X_train = X_train.reshape((X_train.shape[0], 28 * 28 * 1))
X_test = X_test.reshape((X_test.shape[0], 28 * 28 * 1))
X_train = X_train / 255.0
X_test = X_test / 255.0
lb = LabelBinarizer()
Y_train = lb.fit_transform(Y_train)
Y_test = lb.transform(Y_test)
model = Sequential()
model.add(Dense(128, input_shape=(784,), activation="relu"))
model.add(Dense(64, activation="relu"))
model.add(Dense(10, activation="softmax"))
sgd = SGD(0.02)
epochs=20
model.compile(loss="categorical_crossentropy", optimizer=sgd,metrics=["accuracy"])
H = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=epochs, batch_size=64)
predictions = model.predict(X_test, batch_size=128)
print(classification_report(Y_test.argmax(axis=1),predictions.argmax(axis=1),target_names=[str(x) for x in lb.classes_]))
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, epochs), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, epochs), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, epochs), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, epochs), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()


2

from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.layers import Flatten, Activation, Normalization, MaxPooling2D, Conv2D, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']
x_train = x_train/255.0
x_test = x_test/255.0
model = Sequential()

model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))

model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.5))

model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))    # num_classes = 10

# Checking the model summary
model.summary()
adam = Adam(0.001)
model.compile(optimizer=adam, metrics=['Accuracy'], loss=sparse_categorical_crossentropy)
model.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))
model_eval = model.evaluate(x_test, y_test)
print("Accuracy of model is:", model_eval[1], "%")



3




#importing libraries and dataset
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.losses import MeanSquaredLogarithmicError

PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'
data = pd.read_csv(PATH_TO_DATA, header=None)
data.head()
data.shape
#splitting training and testing dataset
features = data.drop(140, axis=1)
target = data[140]
x_train, x_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, stratify=target
)
train_index = y_train[y_train == 1].index
train_data = x_train.loc[train_index]
#scaling the data using MinMaxScaler
min_max_scaler = MinMaxScaler(feature_range=(0, 1))
x_train_scaled = min_max_scaler.fit_transform(train_data.copy())
x_test_scaled = min_max_scaler.transform(x_test.copy())
#creating autoencoder subclass by extending Model class from keras
class AutoEncoder(Model):
  def __init__(self, output_units, ldim=8):
    super().__init__()
    self.encoder = Sequential([
      Dense(64, activation='relu'),
      Dropout(0.1),
      Dense(32, activation='relu'),
      Dropout(0.1),
      Dense(16, activation='relu'),
      Dropout(0.1),
      Dense(ldim, activation='relu')
    ])
    self.decoder = Sequential([
      Dense(16, activation='relu'),
      Dropout(0.1),
      Dense(32, activation='relu'),
      Dropout(0.1),
      Dense(64, activation='relu'),
      Dropout(0.1),
      Dense(output_units, activation='sigmoid')
    ])
  
  def call(self, inputs):
    encoded = self.encoder(inputs)
    decoded = self.decoder(encoded)
    return decoded
#model configuration
model = AutoEncoder(output_units=x_train_scaled.shape[1])
model.compile(loss='msle', metrics=['mse'], optimizer='adam')
epochs = 20

history = model.fit(
    x_train_scaled,
    x_train_scaled,
    epochs=epochs,
    batch_size=512,
    validation_data=(x_test_scaled, x_test_scaled)
)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epochs')
plt.ylabel('MSLE Loss')
plt.legend(['loss', 'val_loss'])
plt.show()
#finding threshold for anomaly and doing predictions
def find_threshold(model, x_train_scaled):
  reconstructions = model.predict(x_train_scaled)
  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)
  threshold = np.mean(reconstruction_errors.numpy()) \
   + np.std(reconstruction_errors.numpy())
  return threshold

def get_predictions(model, x_test_scaled, threshold):
  predictions = model.predict(x_test_scaled)
  errors = tf.keras.losses.msle(predictions, x_test_scaled)
  anomaly_mask = pd.Series(errors) > threshold
  preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)
  return preds

threshold = find_threshold(model, x_train_scaled)
print(f"Threshold: {threshold}")
#getting accuracy score
predictions = get_predictions(model, x_test_scaled, threshold)
acc = accuracy_score(predictions, y_test)
print("Accuracy of model is:", round(acc*100, 1), "%")


4




import warnings
warnings.filterwarnings("ignore")
from tensorflow.keras.preprocessing import text
from tensorflow.keras.utils import to_categorical as np_utils
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.utils import pad_sequences
import numpy as np
import pandas as pd

data = """Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. 
Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
"""
dl_data = data.split()

#tokenization
tokenizer = text.Tokenizer()
tokenizer.fit_on_texts(dl_data)
word2id = tokenizer.word_index

word2id['PAD'] = 0
id2word = {v:k for k, v in word2id.items()}
wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]

vocab_size = len(word2id)
embed_size = 100
window_size = 2 

print('Vocabulary Size:', vocab_size)
print('Vocabulary Sample:', list(word2id.items())[:10])

#generating (context word, target/label word) pairs
def generate_context_word_pairs(corpus, window_size, vocab_size):
    context_length = window_size*2
    for words in corpus:
        sentence_length = len(words)
        for index, word in enumerate(words):
            context_words = []
            label_word   = []            
            start = index - window_size
            end = index + window_size + 1
            
            context_words.append([words[i] 
                                 for i in range(start, end) 
                                 if 0 <= i < sentence_length 
                                 and i != index])
            label_word.append(word)

            x = pad_sequences(context_words, maxlen=context_length)
            y = np_utils(label_word, vocab_size)
            yield (x, y)
            
i = 0
for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):
    if 0 not in x[0]:
        # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])
    
        if i == 10:
            break
        i += 1

#model building
import tensorflow.keras.backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Lambda

cbow = Sequential()
cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))
cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))
cbow.add(Dense(vocab_size, activation='softmax'))
cbow.compile(loss='categorical_crossentropy', optimizer='adam', metrics=["accuracy"])

print(cbow.summary())

cbow.fit(x, y, epochs=10, verbose=1)

weights = cbow.get_weights()[0]
weights = weights[1:]
print(weights.shape)

pd.DataFrame(weights, index=list(id2word.values())[1:]).head()

from sklearn.metrics.pairwise import euclidean_distances

distance_matrix = euclidean_distances(weights)
print(distance_matrix.shape)


similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] 
                   for search_term in ['networks']}

similar_words



5


import tensorflow
import keras
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Flatten, Conv2D, BatchNormalization, Layer, Dense, Dropout, Input
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.datasets import mnist
from tensorflow.image import resize

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = np.stack((x_train,) * 3, axis=-1)
x_test = np.stack((x_test,) * 3, axis=-1)

x_train = resize(x_train, (32, 32))
x_test = resize(x_test, (32, 32))

x_train = x_train/255.0
x_test = x_test/255.0

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
for layer in vgg_model.layers:
    layer.trainable=False

custom_classifier = Sequential()
custom_classifier.add(Flatten())
custom_classifier.add(Dense(128, activation="relu"))
custom_classifier.add(Dropout(0.4))
custom_classifier.add(Dense(64, activation='relu'))
custom_classifier.add(Dropout(0.4))
custom_classifier.add(Dense(10, activation='softmax'))

model = Sequential([vgg_model, custom_classifier])

model.summary()

model.compile(optimizer='adam', loss=sparse_categorical_crossentropy ,metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5, batch_size=256, validation_data=(x_test, y_test))

for layer in vgg_model.layers[:-4]:
 layer.trainable = True

model.compile(optimizer='adam', loss=sparse_categorical_crossentropy ,metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5, batch_size=256, validation_data=(x_test, y_test))


Here's a breakdown of each code section:

1. MNIST Digit Classification
- Libraries and Data: 
  - `sklearn`, `tensorflow`, `matplotlib`, `numpy` are imported for data processing, model building, and visualization. The MNIST dataset, containing images of handwritten digits (0-9), is loaded and flattened (28x28 to 784 features) for input.
- Data Normalization and Label Binarization:
  - The image pixel values are scaled to the range [0, 1] by dividing by 255. Labels are one-hot encoded using `LabelBinarizer`.
- Model:
  - A simple feed-forward neural network (`Sequential` model) with three layers: Dense(128) -> Dense(64) -> Dense(10) (output layer with softmax).
- Compilation and Training:
  - Optimized using SGD with a learning rate of 0.02. Categorical cross-entropy loss and accuracy metrics are used.
- Evaluation:
  - The model is trained for 20 epochs, and results are plotted using Matplotlib. `classification_report` displays the precision, recall, and F1-score for each class.

2. CIFAR-10 Image Classification
- Libraries and Data:
  - `tensorflow` and `numpy` for data handling and model training. CIFAR-10 dataset with 32x32 color images of 10 classes is loaded and normalized.
- Model:
  - A Convolutional Neural Network (CNN) with multiple `Conv2D` layers, `BatchNormalization`, `MaxPooling2D`, and `Dropout` layers to prevent overfitting. Flattened output connected to Dense layers, ending with a `softmax` layer for classification.
- Compilation and Training:
  - Compiled with Adam optimizer and sparse categorical cross-entropy loss. Trained for 20 epochs with a batch size of 64 and validated on the test set.
- Evaluation:
  - The model's performance on the test set is printed.

3. Autoencoder for ECG Data Anomaly Detection
- Libraries and Data:
  - Data imported from an external CSV file. TensorFlow and Scikit-learn are used for model building and data preprocessing.
- Preprocessing:
  - Features and targets are split. Data is scaled using `MinMaxScaler`.
- Model:
  - A custom `AutoEncoder` class extends the `Model` class in TensorFlow. Contains encoder and decoder with `Dense` layers for dimensionality reduction and reconstruction.
- Training:
  - Compiled using `MeanSquaredLogarithmicError` and trained for 20 epochs.
- Anomaly Detection:
  - Threshold for anomaly detection is calculated based on reconstruction errors. Anomaly predictions are generated based on this threshold, and accuracy is evaluated.
- Visualization:
  - Training and validation loss are plotted to monitor training progress.

4. CBOW Model for NLP (Word Embeddings)
- Libraries and Data:
  - Keras preprocessing, tokenization, and embedding utilities are used. A small sample text is tokenized to create a vocabulary.
- Context-Word Pairs:
  - A generator function creates context-target pairs for training the CBOW (Continuous Bag of Words) model.
- Model:
  - A `Sequential` model with an `Embedding` layer and a `Dense` output layer.
- Training:
  - Trained using categorical cross-entropy to predict target words based on context.
- Word Similarity:
  - Euclidean distances between word embeddings are computed to find similar words.

5. VGG16 Transfer Learning for MNIST Classification
- Libraries and Data:
  - The VGG16 pre-trained model from `keras.applications` is used. MNIST dataset is expanded from grayscale to RGB to fit the VGG16 input format (32x32x3).
- Data Preprocessing:
  - Resizing of images to 32x32 and normalization.
- Model:
  - The VGG16 model is used as a feature extractor (`include_top=False`) without its final dense layers. Additional custom layers can be added to create a final classification head for the MNIST dataset.

Each code block showcases different techniques for various machine learning tasks including image classification (using CNNs and pre-trained networks), NLP (word embedding using CBOW), and anomaly detection (using autoencoders).

